{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHdHvxxyUGDP",
        "outputId": "f73ec6cc-cf63-430c-cc82-b15ef71cf561"
      },
      "outputs": [],
      "source": [
        "#installing the ollama package with pip\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQyJIw-_ESwl",
        "outputId": "86be66f1-d24e-473f-dc7f-ff98dd8306b1"
      },
      "outputs": [],
      "source": [
        "#will likely need this\n",
        "!sudo apt-get install lshw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS7z1RPVD3PP",
        "outputId": "97e1b232-d7f4-4461-edbb-28d7dc7550cf"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "62eXv0sOlnRy"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start the Ollama server and wait for a few seconds to ensure it's ready\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(5)  # Wait for 5 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6dGW-53Fkmm",
        "outputId": "59bca037-8c32-4c7f-f242-02c171d325ed"
      },
      "outputs": [],
      "source": [
        "!ollama run deepseek-r1:7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTQAE6GlGt7h",
        "outputId": "ca1b9bc9-8353-47ac-adf4-0f4ff9b712b8"
      },
      "outputs": [],
      "source": [
        "#the process will likely end after the instalation so we set it up again\n",
        "subprocess.Popen(\"ollama serve\", shell=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7pn2cqnFo9P",
        "outputId": "5877358b-cdd5-449f-a50f-31688dc5d457"
      },
      "outputs": [],
      "source": [
        "from ollama import chat\n",
        "from ollama import ChatResponse\n",
        "\n",
        "response: ChatResponse = chat(model='deepseek-r1:7b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    #change your prompt in content param\n",
        "    'content': 'Why is the sky blue?',\n",
        "  },\n",
        "])\n",
        "print(response['message']['content'])\n",
        "# or access fields directly from the response object\n",
        "print(response.message.content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
